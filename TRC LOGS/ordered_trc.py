#!/usr/bin/env python3
"""
trc_merge_and_convert.py

Accept multiple .trc files (v1.1 and/or v2-like), convert v2 -> v1.1 if needed,
remove duplicates by identical $STARTTIME, merge chronologically, produce a single
v1.1 file (Final_Merged_v1.1.trc) and call FW_Config_checker.py with the result.

Behavior matches the Cases:
1) All v1.1 & multiple -> sort by $STARTTIME, merge, header from earliest file
2) All v1.1 & single -> skip merge, pass that file to FW_Config_checker
3) All v2.x -> convert each, then merge (or pass single converted file)
4) Mixed -> convert v2 -> v1.1 then merge
5) Duplicate $STARTTIME -> drop later duplicates (keep first)
6) After final -> call FW_Config_checker.py with the final TRC path
"""

import os
import re
import sys
import subprocess
import tempfile
from pathlib import Path
import tkinter as tk
from tkinter import filedialog, messagebox
from typing import List, Dict, Optional, Tuple, Any

# Regex patterns
RE_FILEVERSION = re.compile(r"^\s*;\$FILEVERSION\s*=\s*([0-9.]+)\s*$")
RE_STARTTIME = re.compile(r"^\s*;\$STARTTIME\s*=\s*([0-9.]+)\s*$")
# v2-ish line: num time TYPE ID Rx/Tx DLC data...
RE_V2_LINE = re.compile(r"^\s*(\d+)\s+([\d.]+)\s+([A-Za-z]+)\s+([0-9A-Fa-f]{1,8})\s+(Rx|Tx|RX|TX)\s+(\d+)\s*(.*)$")
# legacy line: num) time Rx/Tx ID DLC data...
RE_LEG_LINE = re.compile(r"^\s*(\d+)\)?\s+([\d.]+)\s+(Rx|Tx|RX|TX)\s+([0-9A-Fa-f]{1,8})\s+(\d+)\s*(.*)$")
# loose matcher for scanning messages to determine offset
RE_OFFSET = re.compile(r"^\s*\d+\)?\s+([\d.]+)")

# Formatting helper: desired legacy header template (we will substitute Start time and STARTTIME)
LEGACY_HEADER_TEMPLATE = [
    ";$FILEVERSION=1.1",
    ";$STARTTIME={starttime}",
    ";",
    ";   Start time: {starttime_str}",
    ";   Generated by PCAN-View v5.0.1.822",
    ";",
    ";   Message Number",
    ";   |         Time Offset (ms)",
    ";   |         |        Type",
    ";   |         |        |        ID (hex)",
    ";   |         |        |        |     Data Length",
    ";   |         |        |        |     |   Data Bytes (hex) ...",
    ";   |         |        |        |     |   |",
    ";---+--   ----+----  --+--  ----+---  +  -+ -- -- -- -- -- -- --",
]


# ---------------------------
# Utility functions
# ---------------------------
def read_lines(path: Path) -> List[str]:
    with path.open("r", encoding="utf-8", errors="ignore") as fh:
        return fh.readlines()


def extract_metadata_and_sections(path: Path) -> Dict[str, Any]:
    """
    Returns a dict with keys:
      - version: str or None (e.g. '1.1' or '2.0')
      - starttime: float or None (value from ;$STARTTIME=)
      - starttime_str: string after 'Start time:' comment, raw preserved (if present)
      - header_lines: list[str] header (up to dashed separator inclusive)
      - message_lines: list[str] the rest (stripped of newline)
    """
    lines = read_lines(path)
    version = None
    starttime = None
    starttime_str = None
    header_lines: List[str] = []
    message_lines: List[str] = []
    in_header = True

    for ln in lines:
        if in_header:
            header_lines.append(ln.rstrip("\n"))
        # parse version / starttime anywhere
        if version is None:
            m = RE_FILEVERSION.match(ln)
            if m:
                version = m.group(1).strip()
        if starttime is None:
            m2 = RE_STARTTIME.match(ln)
            if m2:
                try:
                    starttime = float(m2.group(1))
                except ValueError:
                    starttime = None
        if starttime_str is None and "Start time:" in ln:
            # preserve literal content to reuse in header
            parts = ln.split("Start time:", 1)
            if len(parts) > 1:
                starttime_str = parts[1].strip()
        # detect header/data separation
        if in_header and ln.strip().startswith(";---"):
            in_header = False
            continue
        if not in_header:
            if ln.strip() != "":
                message_lines.append(ln.rstrip("\n"))

    return {
        "path": str(path),
        "version": version,
        "starttime": starttime,
        "starttime_str": starttime_str,
        "header_lines": header_lines,
        "message_lines": message_lines,
    }


def parse_message_line(line: str) -> Optional[Dict[str, Any]]:
    """
    Try to parse either v2 or legacy line, return dict:
      {'orig_num':int, 'offset_ms':float, 'direction':'Rx'/'Tx', 'id':str, 'dlc':int, 'data':str}
    If cannot parse, return None.
    """
    s = line.strip()
    m2 = RE_V2_LINE.match(s)
    if m2:
        orig_num = int(m2.group(1))
        offset_ms = float(m2.group(2))
        # type token in m2.group(3) is ignored (DT)
        can_id = m2.group(4).upper()
        direction = m2.group(5)
        dlc = int(m2.group(6))
        data = " ".join(m2.group(7).strip().split()) if m2.group(7) else ""
        return {
            "orig_num": orig_num,
            "offset_ms": offset_ms,
            "direction": direction,
            "id": can_id,
            "dlc": dlc,
            "data": data,
        }
    m1 = RE_LEG_LINE.match(s)
    if m1:
        orig_num = int(m1.group(1))
        offset_ms = float(m1.group(2))
        direction = m1.group(3)
        can_id = m1.group(4).upper()
        dlc = int(m1.group(5))
        data = " ".join(m1.group(6).strip().split()) if m1.group(6) else ""
        return {
            "orig_num": orig_num,
            "offset_ms": offset_ms,
            "direction": direction,
            "id": can_id,
            "dlc": dlc,
            "data": data,
        }
    return None


def format_legacy_line(msg_index: int, time_offset_ms: float, direction: str, can_id: str, dlc: int, data: str, msgnum_width: int) -> str:
    """
    Create a well aligned legacy-style line:
    Example:
      "     1)     2.600  Rx       0403  8  00 00 00 00 00 00 00 00"
    Rules:
      - message number field width = msgnum_width (right aligned), then ')', then TWO spaces
      - time offset right-aligned formatted with 3 decimals and width 10 (like 1208762.662)
      - TWO spaces
      - direction 'Rx'/'Tx' left aligned width 2
      - 7 spaces gap to position ID column (matching earlier examples)
      - ID right aligned width 4 (hex)
      - two spaces, dlc, two spaces, data bytes
    """
    dir_norm = "Rx" if direction.lower().startswith("r") else "Tx"
    id_s = str(can_id).upper()
    data_norm = " ".join(data.strip().split()) if data else ""
    return (
        f"{msg_index:>{msgnum_width}})"    # number + )
        f"  "                               # two spaces
        f"{time_offset_ms:>10.3f}"          # time offset width 10 with 3 decimals
        f"  "                               # two spaces
        f"{dir_norm:<2}"                    # direction (2 chars)
        f"{'':7}"                           # 7 spaces gap
        f"{id_s:>4}"                        # ID width 4
        f"  {dlc:>1}  "                     # dlc and two spaces
        f"{data_norm}"
    )


# ---------------------------
# Conversion v2 -> v1.1 (creates converted file next to original)
# ---------------------------
def convert_file_to_v11(src_path: str) -> str:
    """
    Convert a single file from v2-ish to v1.1 style. Writes converted_<orig>.trc
    Returns path to converted file.
    If the file already appears to be v1.1 (has ;$FILEVERSION=1.1 and legacy header),
    returns original path unchanged.
    """
    p = Path(src_path)
    lines = read_lines(p)

    # quick check if already v1.1 with legacy header
    if any(RE_FILEVERSION.match(ln) and RE_FILEVERSION.match(ln).group(1) == "1.1" for ln in lines) and any("Message Number" in ln for ln in lines):
        return src_path

    metadata = extract_metadata_and_sections(p)
    starttime = metadata["starttime"] if metadata["starttime"] is not None else 0.0
    starttime_str = metadata["starttime_str"] or ""
    header = [ln.format(starttime=starttime, starttime_str=starttime_str) if "{" in ln else ln for ln in LEGACY_HEADER_TEMPLATE]
    # we will build messages by parsing original message lines and formatting them
    parsed_msgs: List[Dict[str, Any]] = []
    for raw in metadata["message_lines"]:
        parsed = parse_message_line(raw)
        if parsed:
            parsed_msgs.append(parsed)

    # if no parses found, just return original (can't convert)
    if not parsed_msgs:
        return src_path

    # create converted filename
    converted_name = p.with_name(p.stem + "_converted.trc")
    with converted_name.open("w", encoding="utf-8") as out:
        # write header (replace placeholders)
        for ln in header:
            if isinstance(ln, str) and "{starttime" in ln:
                # shouldn't happen because we replaced above, but safe
                out.write(ln + "\n")
            else:
                out.write(ln + "\n")
        # now write messages mapped to same offsets (we don't re-calc absolute times here)
        # message indices will be as encountered
        idx = 1
        # Determine message number width for this single-file conversion
        num_width = max(6, len(str(len(parsed_msgs))))
        for pm in parsed_msgs:
            out_line = format_legacy_line(idx, pm["offset_ms"], pm["direction"], pm["id"], pm["dlc"], pm["data"], num_width)
            out.write(out_line + "\n")
            idx += 1

    return str(converted_name)


# ---------------------------
# Merge multiple files (after conversion to v1.1 if necessary)
# ---------------------------
def merge_files_chronologically(file_info_list: List[Dict[str, Any]], out_dir: Optional[str] = None) -> str:
    """
    file_info_list: list of dicts produced by extract_metadata_and_sections() OR by reading converted files.
    Returns path to final merged TRC path.
    Behavior:
      - Remove duplicates by identical starttime (keep first occurrence in input list)
      - Compute absolute timestamp for every message: abs_time = file.starttime + offset_ms/1000.0
      - Merge all messages across files and sort by abs_time
      - Re-number sequentially from 1
      - Produce Final_Merged_v1.1.trc in out_dir (or directory of first file)
      - Header is taken from earliest file (by starttime)
    """
    # remove duplicates by identical starttime (keep first occurrence)
    unique_by_start = {}
    ordered_list: List[Dict[str, Any]] = []
    for info in file_info_list:
        st = info.get("starttime")
        if st is None:
            # treat missing starttime as unique by object id
            key = object()
            unique_by_start[key] = info
            ordered_list.append(info)
        elif st not in unique_by_start:
            unique_by_start[st] = info
            ordered_list.append(info)
        else:
            # duplicate found, skip (keep earlier)
            print(f"Skipping duplicate file with identical STARTTIME: {info.get('path')}")

    if not ordered_list:
        raise RuntimeError("No files to merge after removing duplicates.")

    # sort by starttime (None go to end)
    ordered_list.sort(key=lambda x: (float(x["starttime"]) if x["starttime"] is not None else float("inf")))

    # pick header from earliest file
    earliest = ordered_list[0]
    global_start = earliest["starttime"] if earliest["starttime"] is not None else 0.0
    # collect all messages as tuples (abs_time_seconds, original_info, parsed_dict)
    merged_msgs: List[Tuple[float, Dict[str, Any], Dict[str, Any]]] = []

    for info in ordered_list:
        st = info["starttime"] if info["starttime"] is not None else 0.0
        for raw in info["message_lines"]:
            parsed = parse_message_line(raw)
            if not parsed:
                continue
            abs_time = st + (parsed["offset_ms"] / 1000.0)
            merged_msgs.append((abs_time, info, parsed))

    if not merged_msgs:
        raise RuntimeError("No message lines parsed from provided files.")

    # sort messages by abs_time
    merged_msgs.sort(key=lambda x: x[0])

    # final numbering and formatting
    total_messages = len(merged_msgs)
    msgnum_width = max(6, len(str(total_messages)))  # dynamic width
    out_dir_path = Path(out_dir) if out_dir else Path(ordered_list[0]["path"]).parent
    final_name = "Final_Merged_v1.1.trc"
    final_path = out_dir_path / final_name

    # Build header lines (use earliest header, but ensure version=1.1 and STARTTIME=global_start)
    header_lines = []
    for h in earliest["header_lines"]:
        if RE_FILEVERSION.match(h):
            header_lines.append(";$FILEVERSION=1.1")
        elif RE_STARTTIME.match(h):
            header_lines.append(f";$STARTTIME={global_start}")
        else:
            header_lines.append(h)
    # If header didn't include the dashed separator, insert canonical header
    dashed_present = any(line.strip().startswith(";---") for line in header_lines)
    if not dashed_present:
        # append canonical header block
        header_lines.extend([ln.format(starttime=global_start, starttime_str=earliest.get("starttime_str") or "") if "{starttime" in ln else ln for ln in LEGACY_HEADER_TEMPLATE])

    # write final file
    with final_path.open("w", encoding="utf-8") as out:
        for hl in header_lines:
            out.write(hl.rstrip("\n") + "\n")
        out.write("\n")
        # messages
        for idx, (abs_time, info, parsed) in enumerate(merged_msgs, start=1):
            new_offset_ms = (abs_time - global_start) * 1000.0
            line = format_legacy_line(idx, new_offset_ms, parsed["direction"], parsed["id"], parsed["dlc"], parsed["data"], msgnum_width)
            out.write(line + "\n")

    return str(final_path)


# ---------------------------
# Main orchestration
# ---------------------------
def process_and_merge_trcs(paths: List[str]) -> Optional[str]:
    """
    Given file paths (one or more), apply the case logic and return final TRC path.
    """

    # normalize paths
    paths = [str(Path(p)) for p in paths if str(p).lower().endswith(".trc")]
    if not paths:
        print("No .trc files provided.")
        return None

    # Extract info for each
    infos: List[Dict[str, Any]] = []
    for p in paths:
        info = extract_metadata_and_sections(Path(p))
        infos.append(info)

    # Determine versions
    versions = [info.get("version") for info in infos]
    # treat None as not v1.1
    all_v11 = all(v == "1.1" for v in versions)
    all_v2 = all(v is not None and not v.startswith("1.1") for v in versions)  # rough: anything not 1.1 treated as v2-ish
    mixed = not (all_v11 or all_v2)

    # Remove duplicates with identical starttime (keep first in original selection order)
    seen_start = set()
    unique_infos: List[Dict[str, Any]] = []
    for info in infos:
        st = info.get("starttime")
        if st in seen_start:
            print(f"Skipping duplicate (same $STARTTIME): {info.get('path')}")
            continue
        seen_start.add(st)
        unique_infos.append(info)

    if not unique_infos:
        print("All files were duplicates by STARTTIME; nothing to merge.")
        return None

    # CASE 2: single file already v1.1 -> pass through
    if len(unique_infos) == 1 and unique_infos[0].get("version") == "1.1":
        final_path = unique_infos[0]["path"]
        print(f"Single v1.1 file provided; skipping merge. Final: {final_path}")
        return final_path

    # For remaining cases, ensure files are converted to v1.1 (if needed)
    converted_infos: List[Dict[str, Any]] = []
    temp_converted_paths: List[str] = []
    for info in unique_infos:
        if info.get("version") == "1.1":
            converted_infos.append(info)
        else:
            # convert file
            print(f"Converting to v1.1: {info['path']}")
            conv_path = convert_file_to_v11(info["path"])
            temp_converted_paths.append(conv_path)
            # re-extract metadata for converted file
            conv_info = extract_metadata_and_sections(Path(conv_path))
            converted_infos.append(conv_info)

    # If after conversion there is only one file, return that (pass-through)
    if len(converted_infos) == 1:
        final_path = converted_infos[0]["path"]
        print(f"Single file after conversion: {final_path}")
        return final_path

    # Otherwise merge them chronologically
    final_path = merge_files_chronologically(converted_infos)
    print(f"Merge complete. Final file: {final_path}")
    return final_path


def call_fw_checker(trc_path: str) -> None:
    """
    Call FW_Config_checker.py with trc_path as argument.
    If not present in cwd, tries to run it via python -m (best-effort).
    """
    if not trc_path or not os.path.exists(trc_path):
        print("No final TRC path to pass to FW_Config_checker.")
        return
    checker = Path("FW_Config_checker.py")
    try:
        if checker.exists():
            print(f"Calling FW_Config_checker.py with: {trc_path}")
            subprocess.run([sys.executable, str(checker), trc_path], check=False)
        else:
            # try module style
            print("FW_Config_checker.py not found in current dir. Skipping call.")
    except Exception as e:
        print(f"Error calling FW_Config_checker: {e}")


def main_gui():
    root = tk.Tk()
    root.withdraw()
    paths = filedialog.askopenfilenames(title="Select TRC files to merge/convert", filetypes=[("TRC files", "*.trc"), ("All files", "*.*")])
    if not paths:
        print("No files selected.")
        return
    final = process_and_merge_trcs(list(paths))
    if final:
        call_fw_checker(final)
        messagebox.showinfo("TRC Merge", f"Final TRC ready:\n{final}")
    else:
        messagebox.showwarning("TRC Merge", "No final TRC produced.")


if __name__ == "__main__":
    main_gui()
